{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1slD5brLFPm4GqMD7A9Y7nzgdXAAOLUfs",
      "authorship_tag": "ABX9TyNzlZbEm6HL4YUv29rSy8zs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowriks12/Qna-Chatbot/blob/hfmodels/QnaBot_Endpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flask End Point to Access QnaBOT"
      ],
      "metadata": {
        "id": "_Xk-KSNlrCcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waup8m9AYd7D"
      },
      "outputs": [],
      "source": [
        "!pip install flask_ngrok\n",
        "!pip install pyngrok==4.1.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "# !pip install pinecone-client\n",
        "\n",
        "!pip install tiktoken\n",
        "!pip install pyPDF2\n",
        "!pip install HuggingFace\n",
        "!pip install InstructorEmbedding\n",
        "!pip install sentence_transformers\n",
        "!pip -qqq install bitsandbytes accelerate\n",
        "!pip install torch\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "VCbIwCKcZyfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "tgwJ7PUNeVbs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "print(os.environ.get('HUGGINGFACEHUB_API_TOKEN'))"
      ],
      "metadata": {
        "id": "XQu71W3beb67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is NOT available. Using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBq4JM9gefzi",
        "outputId": "a5a18696-b01e-4d4b-f4d3-cb3a0417a320"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pdf_text(pdf_files):\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "    # pdf_file = \"/content/drive/MyDrive/Colab Notebooks/ChatLLMSearch/WCBasicAdminGuide.pdf\"\n",
        "      reader = PdfReader(pdf_file)\n",
        "      for page in reader.pages:\n",
        "          text += page.extract_text()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "uhmRMRSreiJv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunk_text(text):\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "hWDTXSayekw8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_store(text_chunks):\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})\n",
        "\n",
        "    vectorstore = FAISS.from_texts(texts = text_chunks, embedding = embeddings)\n",
        "\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "UyN081TJenmw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OpenAI==0.28.1"
      ],
      "metadata": {
        "id": "8rsjH0Fdojwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def get_conversation_chain(vector_store):\n",
        "\n",
        "    # HuggingFace Model\n",
        "    model = \"tiiuae/falcon-7b-instruct\"\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model,\n",
        "        trust_remote_code=True,\n",
        "        load_in_8bit=True,\n",
        "        device_map=device\n",
        "    )\n",
        "    # Set to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    generate_text = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n",
        "                         trust_remote_code=True, max_new_tokens=100,\n",
        "                         repetition_penalty=1.1, model_kwargs={\"device_map\": \"auto\",\n",
        "                          \"max_length\": 1500, \"temperature\": 0.01, \"torch_dtype\":torch.bfloat16}\n",
        "    )\n",
        "    # LangChain HuggingFacePipeline set to our transformer pipeline\n",
        "    hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "    conversation_chain = RetrievalQA.from_chain_type(llm=hf_pipeline, chain_type=\"stuff\",\n",
        "                                 retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                 return_source_documents=True,\n",
        "                                 verbose=False,\n",
        "    )\n",
        "\n",
        "    return conversation_chain"
      ],
      "metadata": {
        "id": "Y_PlT5Vjer08"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files = [\"/content/drive/MyDrive/Colab Notebooks/ChatLLMSearch/Document-Content-2.pdf\", \"/content/drive/MyDrive/Colab Notebooks/ChatLLMSearch/WCBasicAdminGuide.pdf\"]\n",
        "raw_text = get_pdf_text(pdf_files)\n",
        "# Get Text Chunks\n",
        "text_chunks = get_chunk_text(raw_text)\n",
        "vector_store = get_vector_store(text_chunks)"
      ],
      "metadata": {
        "id": "j-GMTiXreu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# open_api_key = os.environ.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "print(os.environ.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "C9Sf1zorpYgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "llm_openai = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages= True)\n",
        "chain_openai = ConversationalRetrievalChain.from_llm(llm_openai, retriever= vector_store.as_retriever(), memory= memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5s_qJieokzo",
        "outputId": "5759cb81-1dcb-4548-c974-647279d8d04b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = get_conversation_chain(vector_store)"
      ],
      "metadata": {
        "id": "znfzBFZce0Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2apk0ZpGHFhBgGlPmSAx9aGGf4w_28pLAc5WxHqbi1dCJjxFU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9TbIavsZobr",
        "outputId": "a05f2308-b21a-456e-9aae-29561d59c07b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2apk0ZpGHFhBgGlPmSAx9aGGf4w_28pLAc5WxHqbi1dCJjxFU\")\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\", methods=['POST'])\n",
        "def home():\n",
        "  data = request.get_json()\n",
        "  print(data)\n",
        "  question = data.get('question')\n",
        "  print(\"Question: \",question)\n",
        "\n",
        "  if question:\n",
        "    ans = chain({\"query\": question})\n",
        "    answer = ans['result']\n",
        "    # answer = chain_openai.run({\"question\": question})\n",
        "    # answer = \"Answerrrrrr\"\n",
        "      # return jsonify({'answer': answer})\n",
        "  else:\n",
        "    answer = \"ERRRRRR\"\n",
        "    print(\"errr\")\n",
        "      # return jsonify({'error': 'Invalid input'}), 400\n",
        "  return jsonify({'answer' : answer})\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lymw4ezYoq5",
        "outputId": "f4eb4ece-952b-4a5c-d153-256ef77d53b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://828b-34-31-81-206.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Question: What is change notice?'}\n",
            "Question:  Question: What is change notice?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Jan/2024 18:34:45] \"POST / HTTP/1.1\" 200 -\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Question: provide an example scenario of change process'}\n",
            "Question:  Question: provide an example scenario of change process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Jan/2024 18:35:37] \"POST / HTTP/1.1\" 200 -\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Question: ways to convert an eBOM into mBOM'}\n",
            "Question:  Question: ways to convert an eBOM into mBOM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Jan/2024 18:36:54] \"POST / HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-_gcvBnZKqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}